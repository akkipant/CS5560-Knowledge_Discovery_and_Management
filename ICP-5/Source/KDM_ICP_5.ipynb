{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KDM_ICP-5.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNh8DLLin06W6EyM+xVRxkw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akkipant/CS5560-Knowledge_Discovery_and_Management/blob/main/ICP-5/Source/KDM_ICP_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "st-3erh5VE7E"
      },
      "source": [
        "## Task 1: Create 5 separate text files containing text data (blogs,news articles etc)and perform the following Tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzFR_Z6xWChU"
      },
      "source": [
        "### Load the five text files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unUMsdfjVB2G"
      },
      "source": [
        "import os\r\n",
        "files_list = []\r\n",
        "for root, dirs, files in os.walk('Text'):\r\n",
        "    for file in files:\r\n",
        "        if file.endswith('.txt'):\r\n",
        "            with open(os.path.join(root, file), 'r', encoding='utf-8-sig') as f:\r\n",
        "                text = f.read()\r\n",
        "                files_list.append(text)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdGO55a9Xm4f"
      },
      "source": [
        "### Task 1.a) Find out the top10 TF-IDF words for the above input.\r\n",
        "\r\n",
        "---\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umddVJh2ijla",
        "outputId": "ecec24ce-5f81-4d99-fb4b-aa881ba9ea48"
      },
      "source": [
        "#Removing Stop Words\r\n",
        "new_list = []\r\n",
        "import nltk\r\n",
        "nltk.download('stopwords')\r\n",
        "for text in files_list:\r\n",
        "  text = ' '.join([word for word in text.split() if word not in (nltk.corpus.stopwords.words('english'))])\r\n",
        "  new_list.append(text)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuNg8XxeWyuG"
      },
      "source": [
        "import pandas as pd\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "\r\n",
        "#create a tfidfVectorizer\r\n",
        "vectorizer = TfidfVectorizer()\r\n",
        "vectors = vectorizer.fit_transform(new_list)\r\n",
        "#extracting the feature name\r\n",
        "feature_names = vectorizer.get_feature_names()\r\n",
        "dense = vectors.todense()\r\n",
        "denselist = dense.tolist()\r\n",
        "df = pd.DataFrame(denselist, columns=feature_names)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85vljVjUX2ds",
        "outputId": "4a0fd0a7-8947-42c5-e05b-d47129ce8195"
      },
      "source": [
        "df.loc['Total'] = df.sum() # adding row to value total\r\n",
        "\r\n",
        "#filtering values of words whos tfidf is greater than 0.3\r\n",
        "topTen = df.T.sort_values('Total',).tail(10)\r\n",
        "print ()"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                0         1         2         3         4     Total\n",
            "plan     0.000000  0.000000  0.000000  0.000000  0.261246  0.261246\n",
            "max      0.000000  0.000000  0.000000  0.000000  0.261246  0.261246\n",
            "may      0.202730  0.000000  0.094623  0.000000  0.000000  0.297353\n",
            "data     0.000000  0.000000  0.094623  0.000000  0.210771  0.305394\n",
            "one      0.000000  0.000000  0.000000  0.333552  0.000000  0.333552\n",
            "macs     0.000000  0.000000  0.000000  0.333552  0.000000  0.333552\n",
            "arch     0.000000  0.348366  0.000000  0.000000  0.000000  0.348366\n",
            "based    0.000000  0.281060  0.000000  0.000000  0.105386  0.386446\n",
            "company  0.168284  0.000000  0.157091  0.000000  0.087480  0.412855\n",
            "linux    0.000000  0.464488  0.000000  0.000000  0.000000  0.464488\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykI0gA6vXyZC"
      },
      "source": [
        "### Task 1.b) Find out the top10 TF-IDF words for the lemmatized input.\r\n",
        "\r\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fAhiYH-8fT6k",
        "outputId": "de1577a5-35e4-400b-a306-c1dfc770b248"
      },
      "source": [
        "import nltk;nltk.download('punkt');nltk.download('wordnet')\r\n",
        "from nltk.stem import WordNetLemmatizer\r\n",
        "lemmatizer = WordNetLemmatizer()\r\n",
        "\r\n",
        "new_listB = []\r\n",
        "for text in new_list:\r\n",
        "  #Lemmatizing the words\r\n",
        "  text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\r\n",
        "  new_listB.append(text)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "LYJGjgFhZAe6",
        "outputId": "966577e3-88e3-4164-d2b5-63db23311281"
      },
      "source": [
        "lemmatizer.lemmatize('countries')"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'country'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Ly6xaeqZQdk",
        "outputId": "078befe1-acd3-49f4-b8eb-a76a815469bf"
      },
      "source": [
        "# using sklearn library which has inbuilt Tfidf vectorizer class which can generate tfidf for given corpus\r\n",
        "vect = TfidfVectorizer()\r\n",
        "#created TfidfVectorizer object\r\n",
        "tfidf_matrix = vect.fit_transform(new_listB)\r\n",
        "#passed list of documents or corpus to obt method fit_transform\r\n",
        "df = pd.DataFrame(tfidf_matrix.toarray(), columns = vect.get_feature_names())\r\n",
        "# converted method output to panda data frame \r\n",
        "\r\n",
        "df.loc['Total'] = df.sum() # adding row to value total\r\n",
        "\r\n",
        "#filtering values of words whos tfidf is greater than 0.3\r\n",
        "print (df.T.sort_values('Total').tail(10))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                   0         1         2         3         4     Total\n",
            "plan        0.000000  0.000000  0.000000  0.000000  0.262027  0.262027\n",
            "may         0.197129  0.000000  0.093348  0.000000  0.000000  0.290477\n",
            "data        0.000000  0.000000  0.093348  0.000000  0.211402  0.304750\n",
            "macs        0.000000  0.000000  0.000000  0.324645  0.000000  0.324645\n",
            "one         0.000000  0.000000  0.000000  0.324645  0.000000  0.324645\n",
            "researcher  0.000000  0.000000  0.000000  0.324645  0.000000  0.324645\n",
            "arch        0.000000  0.348366  0.000000  0.000000  0.000000  0.348366\n",
            "based       0.000000  0.281060  0.000000  0.000000  0.105701  0.386761\n",
            "company     0.163635  0.000000  0.154974  0.000000  0.087741  0.406350\n",
            "linux       0.000000  0.464488  0.000000  0.000000  0.000000  0.464488\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZ0783y0a-EN"
      },
      "source": [
        "### Task 1.c) Find out the top10 TF-IDF words for the n-gram based input.\r\n",
        "\r\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0vi8MxxdjRo"
      },
      "source": [
        "# this function takes document and n int value to generate list of n grams\r\n",
        "def ngrams(input, n):\r\n",
        "    input = input.split(' ')\r\n",
        "    output = []\r\n",
        "    for i in range(len(input)-n+1):\r\n",
        "        output.append(input[i:i+n])\r\n",
        "    return output"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMmbzMM3Zy8i"
      },
      "source": [
        "new_listC = []\r\n",
        "for text in new_list:\r\n",
        "  text = ' '.join([' '.join(t) for t in ngrams(text, 5)])\r\n",
        "  new_listC.append(text)"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mohAPIZppP9J",
        "outputId": "c7d63693-f1c9-43d5-820e-89642324e072"
      },
      "source": [
        "# using sklearn library which has inbuilt Tfidf vectorizer class which can generate tfidf for given corpus\r\n",
        "vect = TfidfVectorizer( ngram_range=(5,5)) # TfidfVectorizer has inbuilt ngram kwarg which show tfidf for ngrams\r\n",
        "#created TfidfVectorizer object\r\n",
        "tfidf_matrix = vect.fit_transform(new_listC)\r\n",
        "#passed list of documents or corpus to obt method fit_transform\r\n",
        "df = pd.DataFrame(tfidf_matrix.toarray(), columns = vect.get_feature_names())\r\n",
        "# converted method output to panda data frame \r\n",
        "\r\n",
        "df.loc['Total'] = df.sum() # adding row to value total\r\n",
        "\r\n",
        "#filtering values of words whos tfidf is greater than 0.3\r\n",
        "print (df.T.sort_values('Total').tail(10))"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                   0         1  ...    4     Total\n",
            "salientos another interesting arch based    0.000000  0.125988  ...  0.0  0.125988\n",
            "distribution benchmarks showing easy quick  0.000000  0.125988  ...  0.0  0.125988\n",
            "interesting arch based linux distribution   0.000000  0.125988  ...  0.0  0.125988\n",
            "29 139 macs across 150                      0.000000  0.000000  ...  0.0  0.163846\n",
            "activity cluster infected 29 139            0.000000  0.000000  ...  0.0  0.163846\n",
            "infected 29 139 macs across                 0.000000  0.000000  ...  0.0  0.163846\n",
            "cluster infected 29 139 macs                0.000000  0.000000  ...  0.0  0.163846\n",
            "glasses replete far too chunky              0.174371  0.000000  ...  0.0  0.174371\n",
            "far too chunky frames maybe                 0.174371  0.000000  ...  0.0  0.174371\n",
            "replete far too chunky frames               0.174371  0.000000  ...  0.0  0.174371\n",
            "\n",
            "[10 rows x 6 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWyapLtQqPVx"
      },
      "source": [
        "## Task 2: Write a simple spark program to read a dataset and find the W2V similar words (words with higher cosine similarity) for the Top10 TF-IDF Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BfynocT-3UB-",
        "outputId": "189158d0-a40e-4bb7-f6ad-1e9de151d74b"
      },
      "source": [
        "!pip install pyspark"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/67/5158f846202d7f012d1c9ca21c3549a58fd3c6707ae8ee823adcaca6473c/pyspark-3.0.2.tar.gz (204.8MB)\n",
            "\u001b[K     |████████████████████████████████| 204.8MB 57kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 36.8MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.0.2-py2.py3-none-any.whl size=205186687 sha256=42d6bfa873b2ef6d3166fc659cb4577ab6186d83f57deaa315521b3747f0e340\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/09/da/c1f2859bcc86375dc972c5b6af4881b3603269bcc4c9be5d16\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOglzRQe6In9"
      },
      "source": [
        "### Task 2.a) Try without NLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWX683azpVcv",
        "outputId": "3d58273b-4df9-4b01-90b1-8fd1475bbf61"
      },
      "source": [
        "from __future__ import print_function\r\n",
        "from pyspark import SparkContext\r\n",
        "from pyspark.mllib.feature import HashingTF, IDF\r\n",
        "\r\n",
        "sc = SparkContext.getOrCreate()\r\n",
        "documents = sc.textFile(\"Text/text3.txt\").map(lambda line: line.split(\" \"))\r\n",
        "#making an object of HashingTF for finding TF\r\n",
        "hashingTF = HashingTF(numFeatures=20)\r\n",
        "tf = hashingTF.transform(documents)\r\n",
        "\r\n",
        "tf.cache()\r\n",
        "#creating IDF object\r\n",
        "idf = IDF().fit(tf)\r\n",
        "tfidf = idf.transform(tf)\r\n",
        "\r\n",
        "print(\"TFIDF without NLP:\")\r\n",
        "for each in tfidf.collect():\r\n",
        "    print(each)\r\n",
        "sc.stop()"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TFIDF without NLP:\n",
            "(20,[0,1,3,9,16],[0.0,1.3862943611198906,1.0986122886681098,1.3862943611198906,2.6264062120616996])\n",
            "(20,[0],[0.0])\n",
            "(20,[0,2,3,6,7,8,13,14,15,17,18,19],[0.0,3.58351893845611,4.394449154672439,1.7509374747077997,5.375278407684165,1.3862943611198906,1.0986122886681098,1.3862943611198906,2.772588722239781,3.5018749494155994,1.3862943611198906,1.3862943611198906])\n",
            "(20,[0],[0.0])\n",
            "(20,[0,4,9,12,14,16],[0.0,1.7509374747077997,0.6931471805599453,0.8754687373538999,1.3862943611198906,0.8754687373538999])\n",
            "(20,[0],[0.0])\n",
            "(20,[0,3,4,5,6,9,11,12,13,15,16,17,19],[0.0,2.1972245773362196,0.8754687373538999,1.3862943611198906,0.8754687373538999,0.6931471805599453,2.1972245773362196,0.8754687373538999,1.0986122886681098,1.3862943611198906,0.8754687373538999,3.5018749494155994,1.3862943611198906])\n",
            "(20,[0],[0.0])\n",
            "(20,[0,4,5,6,8,9,11,12,13,16,17,18],[0.0,2.6264062120616996,1.3862943611198906,0.8754687373538999,1.3862943611198906,0.6931471805599453,1.0986122886681098,1.7509374747077997,1.0986122886681098,0.8754687373538999,1.7509374747077997,1.3862943611198906])\n",
            "(20,[0],[0.0])\n",
            "(20,[0,1,4,6,9,10,11,12,17],[0.0,1.3862943611198906,0.8754687373538999,0.8754687373538999,0.6931471805599453,1.791759469228055,1.0986122886681098,0.8754687373538999,0.8754687373538999])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yo9GSMBr6EkV"
      },
      "source": [
        "### Task 2.b) Try with Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9g4GvnL3_WN",
        "outputId": "a4b0dfbe-e6d0-4a2c-e100-d3ac854bf74b"
      },
      "source": [
        "from __future__ import print_function\r\n",
        "\r\n",
        "import nltk\r\n",
        "nltk.download('punkt')\r\n",
        "from pyspark import SparkContext\r\n",
        "from pyspark.mllib.feature import HashingTF, IDF\r\n",
        "from nltk.stem import WordNetLemmatizer\r\n",
        "\r\n",
        "sc = SparkContext.getOrCreate()\r\n",
        "documents = sc.textFile(\"Text/text3.txt\").map(lambda line: line.split(\" \"))\r\n",
        "\r\n",
        "lemmatizer = WordNetLemmatizer()\r\n",
        "\r\n",
        "word_list = list(map(' '.join, documents.collect()))\r\n",
        "word_list1 = ''\r\n",
        "for i in word_list:\r\n",
        "    word_list1 = word_list1 + ' ' + i\r\n",
        "word_list2 = nltk.word_tokenize(word_list1)\r\n",
        "lemmatized_document = ' '.join([lemmatizer.lemmatize(w) for w in word_list2])\r\n",
        "print(lemmatized_document)\r\n",
        "\r\n",
        "f = open(\"test.txt\", \"w+\")\r\n",
        "f.write('' + lemmatized_document)\r\n",
        "f.close()\r\n",
        "\r\n",
        "document1 = sc.textFile(\"Text/text3.txt\").map(lambda line: line.split(\" \"))\r\n",
        "\r\n",
        "hashingTF = HashingTF(numFeatures=20)\r\n",
        "tf = hashingTF.transform(document1)\r\n",
        "tf.cache()\r\n",
        "idf = IDF().fit(tf)\r\n",
        "tfidf = idf.transform(tf)\r\n",
        "\r\n",
        "print(\"TFIDF with Lemmatization:\")\r\n",
        "for each in tfidf.collect():\r\n",
        "    print(each)\r\n",
        "sc.stop()"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "It seems like it wa a weekend of leak . Pick your company : Samsung ’ s concept for AR glass , replete with some far-too-chunky frame , or maybe the next generation of Apple ’ s AirPods , the in-ear kind . These are two very different kind of leak . Samsung ’ s appears to be the sort of conceptual video tease that may , or may not , result in a device . Previous tease resulted in hardware that looked pretty different to the heady dream from year earlier . The company ’ s foldable phone are a great example , here .\n",
            "TFIDF with Lemmatization:\n",
            "(20,[0,1,3,9,16],[0.0,1.3862943611198906,1.0986122886681098,1.3862943611198906,2.6264062120616996])\n",
            "(20,[0],[0.0])\n",
            "(20,[0,2,3,6,7,8,13,14,15,17,18,19],[0.0,3.58351893845611,4.394449154672439,1.7509374747077997,5.375278407684165,1.3862943611198906,1.0986122886681098,1.3862943611198906,2.772588722239781,3.5018749494155994,1.3862943611198906,1.3862943611198906])\n",
            "(20,[0],[0.0])\n",
            "(20,[0,4,9,12,14,16],[0.0,1.7509374747077997,0.6931471805599453,0.8754687373538999,1.3862943611198906,0.8754687373538999])\n",
            "(20,[0],[0.0])\n",
            "(20,[0,3,4,5,6,9,11,12,13,15,16,17,19],[0.0,2.1972245773362196,0.8754687373538999,1.3862943611198906,0.8754687373538999,0.6931471805599453,2.1972245773362196,0.8754687373538999,1.0986122886681098,1.3862943611198906,0.8754687373538999,3.5018749494155994,1.3862943611198906])\n",
            "(20,[0],[0.0])\n",
            "(20,[0,4,5,6,8,9,11,12,13,16,17,18],[0.0,2.6264062120616996,1.3862943611198906,0.8754687373538999,1.3862943611198906,0.6931471805599453,1.0986122886681098,1.7509374747077997,1.0986122886681098,0.8754687373538999,1.7509374747077997,1.3862943611198906])\n",
            "(20,[0],[0.0])\n",
            "(20,[0,1,4,6,9,10,11,12,17],[0.0,1.3862943611198906,0.8754687373538999,0.8754687373538999,0.6931471805599453,1.791759469228055,1.0986122886681098,0.8754687373538999,0.8754687373538999])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUDgoFn-6EEj"
      },
      "source": [
        "### Task 2.c) Try with NGrams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTWlv4pc6F5O",
        "outputId": "f8ce7d2e-728c-417e-c510-7c991c7db6cd"
      },
      "source": [
        "import nltk\r\n",
        "import re\r\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from nltk.tokenize import word_tokenize\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "txt1 = []\r\n",
        "with open('test.txt') as file:\r\n",
        "    txt1 = file.readlines()\r\n",
        "\r\n",
        "def remove_string_special_characters(s):\r\n",
        "    stripped = re.sub('[^a-zA-z\\s]', '', s)\r\n",
        "    stripped = re.sub('_', '', stripped)\r\n",
        "    stripped = re.sub('\\s+', ' ', stripped)\r\n",
        "    stripped = stripped.strip()\r\n",
        "    if stripped != '':\r\n",
        "        return stripped.lower()\r\n",
        "\r\n",
        "stop_words = set(stopwords.words('english'))\r\n",
        "your_list = ['skills', 'ability', 'job', 'description']\r\n",
        "for i, line in enumerate(txt1):\r\n",
        "    txt1[i] = ' '.join([x for\r\n",
        "                        x in nltk.word_tokenize(line) if\r\n",
        "                        (x not in stop_words) and (x not in your_list)])\r\n",
        "\r\n",
        "vectorizer = CountVectorizer(ngram_range=(2, 2))\r\n",
        "X1 = vectorizer.fit_transform(txt1)\r\n",
        "features = (vectorizer.get_feature_names())\r\n",
        "print(\"\\n\\nFeatures : \\n\", features)\r\n",
        "print(\"\\n\\nX1 : \\n\", X1.toarray())\r\n",
        "\r\n",
        "vectorizer = TfidfVectorizer(ngram_range=(3, 3))\r\n",
        "X2 = vectorizer.fit_transform(txt1)\r\n",
        "scores = (X2.toarray())\r\n",
        "print(\"\\n\\nScores : \\n\", scores)\r\n",
        "\r\n",
        "# Getting top ranking features \r\n",
        "sums = X2.sum(axis=0)\r\n",
        "data1 = []\r\n",
        "for col, term in enumerate(features):\r\n",
        "    data1.append((term, sums[0, col]))\r\n",
        "ranking = pd.DataFrame(data1, columns=['term', 'rank'])\r\n",
        "words = (ranking.sort_values('rank', ascending=False))\r\n",
        "print(\"\\n\\nWords head : \\n\", words.head(7))"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Features : \n",
            " ['airpods in', 'appears sort', 'apple airpods', 'ar glass', 'chunky frame', 'company foldable', 'company samsung', 'concept ar', 'conceptual video', 'device previous', 'different heady', 'different kind', 'dream year', 'ear kind', 'earlier the', 'far too', 'foldable phone', 'frame maybe', 'generation apple', 'glass replete', 'great example', 'hardware looked', 'heady dream', 'in ear', 'it seems', 'kind leak', 'kind these', 'leak pick', 'leak samsung', 'like wa', 'looked pretty', 'may may', 'may result', 'maybe next', 'next generation', 'phone great', 'pick company', 'pretty different', 'previous tease', 'replete far', 'result device', 'resulted hardware', 'samsung appears', 'samsung concept', 'seems like', 'sort conceptual', 'tease may', 'tease resulted', 'the company', 'these two', 'too chunky', 'two different', 'video tease', 'wa weekend', 'weekend leak', 'year earlier']\n",
            "\n",
            "\n",
            "X1 : \n",
            " [[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]]\n",
            "\n",
            "\n",
            "Scores : \n",
            " [[0.13483997 0.13483997 0.13483997 0.13483997 0.13483997 0.13483997\n",
            "  0.13483997 0.13483997 0.13483997 0.13483997 0.13483997 0.13483997\n",
            "  0.13483997 0.13483997 0.13483997 0.13483997 0.13483997 0.13483997\n",
            "  0.13483997 0.13483997 0.13483997 0.13483997 0.13483997 0.13483997\n",
            "  0.13483997 0.13483997 0.13483997 0.13483997 0.13483997 0.13483997\n",
            "  0.13483997 0.13483997 0.13483997 0.13483997 0.13483997 0.13483997\n",
            "  0.13483997 0.13483997 0.13483997 0.13483997 0.13483997 0.13483997\n",
            "  0.13483997 0.13483997 0.13483997 0.13483997 0.13483997 0.13483997\n",
            "  0.13483997 0.13483997 0.13483997 0.13483997 0.13483997 0.13483997\n",
            "  0.13483997]]\n",
            "\n",
            "\n",
            "Words head : \n",
            "                term     rank\n",
            "0        airpods in  0.13484\n",
            "1      appears sort  0.13484\n",
            "30    looked pretty  0.13484\n",
            "31          may may  0.13484\n",
            "32       may result  0.13484\n",
            "33       maybe next  0.13484\n",
            "34  next generation  0.13484\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCFXBgbK6xhb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}